{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7b5f75a",
   "metadata": {},
   "source": [
    "### Tensor是什么？\n",
    "在 PyTorch 中，Tensor 是最基本的数据结构，它类似于 NumPy 中的多维数组，但额外支持：\n",
    "- 自动求导（autograd）\n",
    "- GPU 加速（CUDA）\n",
    "- 广播机制（Broadcasting）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3018077b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar: 5 | Shape: torch.Size([])\n",
      "Vector: tensor([1, 2, 3]) | Shape: torch.Size([3])\n",
      "Matrix: tensor([[1, 2],\n",
      "        [3, 4]]) | Shape: torch.Size([2, 2])\n",
      "Tensor: tensor([[[-0.9934, -1.1354,  0.5186,  0.0655],\n",
      "         [-1.2734,  1.3835, -2.8503,  0.8640],\n",
      "         [-1.7711, -0.6974,  0.4171, -0.4670]],\n",
      "\n",
      "        [[-0.9032, -0.8762,  1.1044,  0.7193],\n",
      "         [-1.1354,  0.5581, -0.1455,  0.7685],\n",
      "         [ 0.3139,  0.5544, -0.8425, -0.4920]]]) | Shape: torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Scalar\n",
    "a = torch.tensor(5)\n",
    "print(f'Scalar: {a} | Shape: {a.shape}')\n",
    "\n",
    "# Vector\n",
    "b = torch.tensor([1, 2, 3])\n",
    "print(f'Vector: {b} | Shape: {b.shape}')\n",
    "\n",
    "# Matrix\n",
    "c = torch.tensor([[1,2],[3,4]])\n",
    "print(f'Matrix: {c} | Shape: {c.shape}')\n",
    "\n",
    "# Tensor\n",
    "d = torch.randn(2,3,4)\n",
    "print(f'Tensor: {d} | Shape: {d.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06358d69",
   "metadata": {},
   "source": [
    "- Tensor是PyTorch中的多维数组结构，支持自动求导和GPU加速，属于深度学习的核心计算单位\n",
    "- Tensor的默认设备是CPU，不过可以通过'.cuda()'或者'.to('cuda')'移动到GPU上面\n",
    "- 如何创建一个3*4的Tensor？torch.randn(3,4)\n",
    "- Tensor和Numpy有什么不同？Tensor可以反向传播，可以GPU加速，功能更强"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290278b5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b25d715d",
   "metadata": {},
   "source": [
    "### 改变Tensor的形状"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7157676",
   "metadata": {},
   "source": [
    "在深度学习中，Tensor的形状经常(Shape)经常需要匹配模型的输入结构，比如：\n",
    "- 图像模型：需要输入的结构是(B, C, H, W) -- (batch, channel, height, width)\n",
    "- NLP模型：需要输入的结构是(batchm, seq_length)\n",
    "我们必须熟悉的调整Tensor的形状，而不能生成新的数据，这就view/reshape的用武之地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d09b342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x's original shape: torch.Size([2, 3, 4])\n",
      "x_reshaped shape: torch.Size([6, 4])\n"
     ]
    }
   ],
   "source": [
    "# view() & reshape()\n",
    "x = torch.randn(2,3,4)\n",
    "print(f\"x's original shape: {x.shape}\") # 这里的Tensor有三个维度(batch， height, width)，可以想象城两个3*4的矩阵叠在一起\n",
    "\n",
    "\n",
    "# 使用view()改变形状，把上面的Tensor变成一个6*4的矩阵\n",
    "x_reshaped = x.view(6,4)\n",
    "print(f'x_reshaped shape: {x_reshaped.shape}')\n",
    "\n",
    "### 注意，view要求Tensor在内存中是连续的，但是reshape会自动处理，所以更推荐使用reshape；如果内存不连续，使用view之前需要先使用contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829cf46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x's original shape: torch.Size([3])\n",
      "x_batch shape: torch.Size([1, 3])\n",
      "x_more shape: torch.Size([1, 3, 1])\n",
      "x_back shape: torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "# squeeze() & unsqueeze()\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(f\"x's original shape: {x.shape}\")\n",
    "# 添加一个batch维度，作为一个batch输入模型\n",
    "x_batch= x.unsqueeze(0)\n",
    "print(f'x_batch shape: {x_batch.shape}')\n",
    "# 在添加一个维度\n",
    "x_more = x_batch.unsqueeze(-1)\n",
    "print(f'x_more shape: {x_more.shape}')\n",
    "# 再删除上一步添加的维度\n",
    "x_back = x_more.squeeze(-1)\n",
    "print(f'x_back shape: {x_back.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9390ab8b",
   "metadata": {},
   "source": [
    "### 模型输入通常会用到unsqueeze(0)， 是因为要添加batch维度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509a8722",
   "metadata": {},
   "source": [
    "- view()和reshape()用来改变tensor的维度，reshape更加灵活因为不要求内存连续；unsqueeze()在指定位置添加维度，squeeze删除shape=1的维度\n",
    "- 注意view()要求连续内存，所以为了保证view()不出错，我们可以保持tensor.contiguous().view(.., ..)的形式\n",
    "\n",
    "**Tensor 的 view/reshape 虽然常用于适配模型结构，但我们更应该从数据本身的语义出发来设计输入格式，而不是反过来为了迎合模型而强行 reshape。否则会引入结构错误，比如跨样本 flatten、多维错位等问题。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d4647cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flatten shape: torch.Size([32, 3136])\n",
      "Image shape: torch.Size([1, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# view() & reshape() 的常见实用场景\n",
    "\n",
    "# （1）适用于CNN输出支全连接层\n",
    "x = torch.randn(32, 64, 7, 7)\n",
    "x_flatten = x.contiguous().view(32,-1) # '-1'代表让机器自动计算维度，这里的意思是保留32， -1处自动计算--64*7*7\n",
    "print(f'Flatten shape: {x_flatten.shape}')\n",
    "\n",
    "# （2）增加batch和通道维度\n",
    "img = torch.randn(28, 28) # 目前只有height，width两个维度\n",
    "img = img.unsqueeze(0).unsqueeze(0) # 我们需要增加batch和channel两个维度\n",
    "print(f'Image shape: {img.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d40ce9c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1beb2d14",
   "metadata": {},
   "source": [
    "### Broadcasting机制\n",
    "- broadcasting机制是**右对齐维度**：从后往前比，如果两个tensor的维度不一样，会先在最前面补1，使他们的维度一样。 比如A.shape = (2, 3, 1), B.shape = (3，4)， B会先变成(1,3,4)，然后在进行广播\n",
    "- 然后是每一个维度的比较规则，看是都可以进行braodcasting\n",
    "  - 1.维度大小相等；2.一个是 1，一个不是。 如果两个都不是1且双方不相等，那么不可以broadcasting，直接报错\n",
    "  - broadcasting后的维度为最大维度的组合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5494baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "A = torch.randn(2, 3, 1)\n",
    "B = torch.randn(3, 4)\n",
    "\n",
    "# 自动补维\n",
    "# A: (2, 3, 1)\n",
    "# B: (1, 3, 4)\n",
    "\n",
    "# 比较维度：\n",
    "# (1 vs 4) ✅ → A扩展\n",
    "# (3 vs 3) ✅\n",
    "# (2 vs 1) ✅ → B扩展\n",
    "\n",
    "# 最终广播形状：(2, 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177ad7c3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1d64985",
   "metadata": {},
   "source": [
    "### Autograd, requires_grad, detach和PyTorch的自动微分机制\n",
    "- PyTorch 的 Autograd 模块会自动构建一张计算图（computation graph），追踪 Tensor 的所有运算过程，从而实现自动求导。\n",
    "- 每当你对一个 requires_grad=True 的 Tensor 进行操作时，PyTorch 就在背后记录这些操作，以便之后使用 .backward() 自动计算梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee436f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([4., 6., 8.])\n"
     ]
    }
   ],
   "source": [
    "# requires_grad\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "# requires_grad设置为True，表示这个Tensor需要梯度计算\n",
    "# **requires_grad默认是False**，只做前向传播的计算，不做反向传播\n",
    "\n",
    "\n",
    "\n",
    "# 如何执行反向传播？\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) # 先创建一个Tensor，注意这里的requires_grad必须\n",
    "print(x.grad)\n",
    "y = x**2 + 2*x + 1\n",
    "loss = y.sum()\n",
    "loss.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61363a54",
   "metadata": {},
   "source": [
    "### detach和torch.no_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3dfb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detach可以用来断开计算图\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x*2\n",
    "z = y.detach() # z和y数值形状一样，但是z不参与梯度计算\n",
    "\n",
    "# torch.no_grad可以用来临时关闭autograd\n",
    "with torch.no_grad():\n",
    "    y = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571f500e",
   "metadata": {},
   "source": [
    "- detach() 生成新 Tensor，不追踪；no_grad 是上下文临时禁用\n",
    "- 训练前要 optimizer.zero_grad()，梯度默认会累加，不清空就会出错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21809e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x 的梯度： tensor([4., 6., 8.])\n",
      "y_detached.requires_grad: False\n",
      "z.requires_grad: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个需要梯度的张量\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# 构建前向计算图\n",
    "y = x**2 + 2 * x + 1\n",
    "loss = y.sum()\n",
    "\n",
    "# 执行反向传播\n",
    "loss.backward()\n",
    "\n",
    "# 查看梯度\n",
    "print(\"x 的梯度：\", x.grad)  # 对每个 x_i：d(loss)/dx_i = 2x_i + 2\n",
    "\n",
    "# 使用 detach 获取不追踪梯度的张量\n",
    "y_detached = y.detach()\n",
    "print(\"y_detached.requires_grad:\", y_detached.requires_grad)\n",
    "\n",
    "# 使用 no_grad 环境（推理阶段常用）\n",
    "with torch.no_grad():\n",
    "    z = x * 3\n",
    "    print(\"z.requires_grad:\", z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b343d453",
   "metadata": {},
   "source": [
    "- .backward() 是触发 反向传播 的方法，会从一个标量（通常是 loss）出发，沿着计算图计算每个变量的梯度。\n",
    "- .grad 是每个 Tensor 存储 梯度结果 的属性。\n",
    "- requires_grad=True 的 Tensor 默认在哪里存储梯度？答：在 .grad 属性中。\n",
    "- detach用于提取中间层输出而不追踪梯度，适合特征提取、可视化和冻结模块\n",
    "- 什么时候使用 torch.no_grad()？用在：\n",
    "  - 模型推理阶段（inference），比如部署、测试。\n",
    "  - 只需要前向输出，不需要反向传播和梯度。\n",
    "- model.zero_grad()等价于：\n",
    "for param in model.parameters():\n",
    "    param.grad = None  # 这也是新 PyTorch 推荐的方法，更节省内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e0c4914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detach的实用场景\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
